\documentclass[a4paper,12pt,notitlepage,pdftex,headsepline]{scrartcl}

\usepackage{a4wide}
\usepackage{cmap} % чтобы работал поиск по PDF
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[T2A]{fontenc}

\usepackage{textcase}
\usepackage[pdftex]{graphicx}

\usepackage{lscape}

\pdfcompresslevel=9 % сжимать PDF
\usepackage{pdflscape} % для возможности альбомного размещения некоторых страниц
\usepackage[pdftex]{hyperref}
% настройка ссылок в оглавлении для pdf формата
\hypersetup{unicode=true,
            pdftitle={Курсовая по ЧМ},
            pdfauthor={Погода Михаил, Превир Николай},
            pdfcreator={pdflatex},
            pdfsubject={},
            pdfborder    = {0 0 0},
            bookmarksopen,
            bookmarksnumbered,
            bookmarksopenlevel = 2,
            pdfkeywords={},
            colorlinks=true, % установка цвета ссылок в оглавлении
            citecolor=black,
            filecolor=black,
            linkcolor=black,
            urlcolor=blue}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{moreverb}
%for \includepdf
%\usepackage{pdfpages}

\author{Михаил Погода, Превир Николай}
\title{Курсовая работа по численным методам}
\date{\today}

\def\sign{\mathop{\rm sign}}

\begin{document}
\thispagestyle{empty}
\begin{center}
Министерство образования и науки, молодёжи и спорта Украины\\
Национальный технический университет Украины\\
,,Киевский политехнический институт''\\
(НТУУ ,,КПИ'')\\
\vspace*{4cm}
\MakeUppercase{Курсовая работа}\\
\MakeUppercase{по дисциплине: ,,Численные методы''}\\
\MakeUppercase{Исследование и анализ методов решения алгебраических}\\
\MakeUppercase{проблем собственных значений и их практическое применение}\\
\vspace*{3cm}
Выполнили:\hfill~\\
студенты III курса ФПМ\hfill М.\,В.~ПОГОДА\\
группы КМ-92\hfill 		     Н.\,В.~ПРЕВИР\\
\vspace{2cm}
Руководитель\hfill В.\,В.~МАЛЬЧИКОВ\\
\vspace*{5cm}
2011
\end{center}
\newpage
\tableofcontents
\newpage

\section*{Введение}
\addcontentsline{toc}{section}{Введение}
В современных науках, будь то физика, химия или биология, особое место отведено математическому аппарату.
Если удаётся найти правильную математическую модель для процесса, то этот процесс можно детально смоделировать на ЭВМ.

Зачастую такие модели используют теорию дифференциальных уравнений.
Уравнения, характеризующие модель, как правило, решают с использованием численных методов.

Одним из важных этапов решения системы дифференциальных уравнений является поиск собственных векторов и собственных значений оператора системы.
В данной работе мы рассмотрим численные методы, применимые для оценки области, содержащую собственные числа, и алгоритмы, применимые для поиска собственных чисел и построения характеристического многочлена.
\newpage
\section{Постановка задачи}
Дано матрицу
$$A = \left( \begin{matrix}
7.2 & -2.0 & 0.6 & 0.2\\
-2.0 & 7.3 & 2.0 & -1.5\\
0.6 & 2.0 & 7.5 & 0\\
0.2 & -1.5 & 0 & 8.0
\end{matrix}\right)$$
Для матрицы $A$ необходимо:
\begin{enumerate}
\item Определить область, содержащую собственные числа.
\item Построить характеристический многочлен.
\item Найти собственное число из интервала $\left( 10; 12 \right)$ по многочлену с точностью $0.1$, уточнить его до требуемой точности и найти соответствующий ему собственный вектор.
\item Выполнить 3-4 шага QR-алгоритма поиска собственных чисел с анализом получаемых результатов после каждого шага.
\end{enumerate}
\newpage
\section{Исследование и анализ теоретического материала}
Собственным вектором матрицы
$$A = \left(
\begin{matrix}
a_{11} & a_{12} & \hdots & a_{1n}\\
a_{21} & a_{22} & \hdots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n1} & a_{n2} & \hdots & a_{nn}
\end{matrix}\right)$$
называется ненулевой вектор $\vec{x}$, такой, что

\begin{equation}
\label{eq:ax=lx}
A \vec{x} = \lambda \vec{x}
\end{equation}

Число $\lambda$ называется собственным числом, соответствующее собственному вектору $\vec{x}$.

Чтобы однородная линейная система уравнений (\ref{eq:ax=lx}) имела нетривиальное решение, необходимо, чтобы
\begin{equation}\label{eq:character-eq}
\det\left(\strut A - \lambda I\right) = 0
\end{equation}

Уравнение (\ref{eq:character-eq}) называется характеристическим уравнением матрицы $A$.
\subsection{Теорема Гершгорина}
\label{ss:gershgorin}
Теорема Гершгорина определяет область комплексной плоскости, которая содержит все собственные числа комплексной квадратной матрицы.
Для матрицы $A$ ($n\times n$) определим 
$$R_i = \sum\limits_{\substack{
j = 1\\
j \neq i
}
}^n \left| a_{ij} \right|,\qquad i = 1, 2, \dots, n$$

Тогда каждое собственное число матрицы $A$ находится хотя б в одном из кругов

$$\left\{  z: \left| z - a_{ii}\right| \leqslant R_i \right\}, \qquad i = 1, 2, \dots, n$$
Если все круги не пересекаются, то в каждом из $n$ кругов находится только одно собственное число. 

\subsection{Метод вращений (Якоби)}
\label{ss:jacobi}
Этот метод широко используется для построения характеристического многочлена симметрической матрицы $A$.
Метод основывается на том факте, что подобные матрицы имеют одинаковые характеристические многочлены.
В прямом методе Якоби матрица $A$ преобразуется к трёхдиагональному виду, сохраняя подобие.

Вращением называется преобразование координат с помощью ортогональной матрицы $T_{ij}$, которая представляет собой единичную матрицу $I$, кроме элементов $t_{ii} = t_{jj} = c, t_{ji} = -t_{ij} = s$, где $c^2 + s^2 = 1$.

С помощью цепочки вращений любую симметрическую матрицу можно привести к трёхдиагональному виду, при этом полученная трёхдиагональная матрица также будет симметрической, т.\,к. ортогональное преобразование сохраняет симметрию.

Рассмотрим матрицы $$B = A T_{ij}$$
$$C = T_{ij}^T B = T_{ij}^T A T_{ij}$$

Т.\,к. $T_{ij}^{-1} = T_{ij}^T$, то матрицы $C$ и $A$ подобны.
Далее, легко видеть, что все столбцы матрицы $B$ совпадают со столбцами матрицы $A$, за исключением $i$-го и $j$-го столбцов, которые получаются из соответствующих столбцов матрицы $A$ по формулам:
\begin{eqnarray}
\label{eq:b_ij_from_a_ij}
B_i = c A_i + s A_j\\
B_j = -s A_i + c A_j
\end{eqnarray}

В свою очередь, строки матрицы $C$ совпадают со строками матрицы $B$, за исключением $i$-ой и $j$-ой, которые получаются из соответствующих строк матрицы $B$ по формулам:
\begin{eqnarray}
\label{eq:c_ij_from_b_ij}
C^i = c B^i + s B^j\\
C^j = -s B^i + c B^j
\end{eqnarray}

Пусть $1 < i < j$. Покажем, что $c$ и $s$ можно подобрать так, чтобы $c_{i-1 j}$ = 0.
Действительно, $c_{i-1 j} - b_{i-1 j} = -s a_{i-1 j} + c a_{i-1 j}$.
Из условия $c_{i-1 j} = 0$, получаем $\frac{s}{c} = \frac{a_{i-1 j}}{a_{i-1 i}}$.
Кроме того, $s^2 + c^2 = 1$, поэтому
\begin{eqnarray}
\label{eq:s_and_c_eq}
s = \frac{a_{i-1 j}}{\pm\sqrt{a_{i-1 i}^2 + a_{i-1 j}^2}}\\
c = \frac{a_{i-1 i}}{\pm\sqrt{a_{i-1 i}^2 + a_{i-1 j}^2}}
\end{eqnarray}

Из изложенного вытекает, что процесс трёхдиагонализации матрицы $A$ можно провести следующим образом:
с помощью преобразований $T_{23}, T_{24}, \dots, T_{2n}$ аннулируются по очереди элементы первого строки, начиная с третьего;
затем, с помощью преобразований $T_{34}, T_{35}, \dots, T_{3n}$ аннулируются элементы второй строки, начиная с четвёртого.
Очевидно, что при этом элементы первой строки меняться не будут (первые два элемента первой строки вообще не будут участвовать в преобразованиях, а остальные, равные нулю, элементы, будут подвергаться преобразованиям, но останутся равными нулю).
\dots

Таким образом мы перейдём от данной симметрической матрицы к трёхдиагональной матрице
$$S = \left(\begin{matrix}
s_{11} & s_{12} & 0 & 0 & \hdots & 0\\
s_{21} & s_{22} & s_{23} & 0 & \hdots & 0\\
0 & s_{32} & s_{33} & s_{34} & \hdots & 0\\
\vdots & \vdots & \ddots & \ddots & \ddots & \vdots\\
0 & 0 & 0 & \hdots & s_{n n-1} & s_{nn}
\end{matrix}\right)$$

Причём $S = S^T$ и $S = T^{-1} A T$, т.\,е. матрицы $A$ и $S$ подобны.
Для характеристического многочлена $\varphi\left(t\right)$ трёхдиагональной матрицы $S$ можно построить рукуррентную формулу:
$$\varphi_0\left(\lambda\right) = 1$$
$$\varphi_1\left(\lambda\right) = \lambda - s_{11}$$
$$\varphi_2\left(\lambda\right) = \left(\lambda - s_{22}\right) - s_{12}^2\varphi_0\left(\lambda\right)$$
$$\dots$$
$$\varphi_i\left(\lambda\right) = \left(\lambda - s_{ii}\right) - s_{i-1 i}^2\varphi_{i - 2}\left(\lambda\right)$$

\begin{equation}
\label{eq:char-eq-formulae}
\varphi\left(\lambda\right) = \left(-1\right)^n \varphi_n\left(\lambda\right)
\end{equation}
\subsection{Обратный степенной метод со сдвигом}
\label{ss:reverse}
Итерации, определяемые формулой
\begin{equation}
\label{eq:reverse-method}
\vec{x}^{\left(s + 1\right)} = A^{-1} \vec{x}^{\left(s\right)}
\end{equation}
, начиная с произвольного $\vec{x}^{\left(0\right)}$, сходятся к наибольшему по модулю собственному значению матрицы $A^{-1}$ и, значит, к наименьшему по модулю собственному значению матрицы $A$.
Сходимость, как правило, будет медленной.
Однако, с помощью сдвига сходимость можно существенно улучшить.
Действительно, пусть нам известно приближённо некоторое, не обязательно наименьшее, собственное значение $\tilde{\lambda_k}$.
Тогда сдвинутая матрица $A - \tilde{\lambda_k} I$ будет иметь собственные значения $\lambda_i - \tilde{\lambda_k}$.
Для матрицы $A - \tilde{\lambda_k} I$ собственное значение $\lambda_k - \tilde{\lambda_k}$ будет наименьшим по модулю.
Обратные итерации со сдвинутой матрицей, которые мы теперь запишем в виде
\begin{equation}
\label{eq:reverse-iteration}
\left(A - \tilde{\lambda_k} I\right) \vec{x}^{\left(s+1\right)} = \vec{x}^{\left(s\right)}
\end{equation}
, будут сходиться и определят искомое собственное значение $\left(\lambda_k - \tilde{\lambda_k}\right)^{-1}$.
Если $\tilde{\lambda_k}$ --- достаточно хорошее приближение для $\lambda_k$, то сходимость достаточно быстрая (иногда процесс сходится за несколько итераций).
Как и в обычном степенном методе, после каждой итерации надо нормировать вектор, чтобы избежать машинных нулей и переполнений.

Если сдвиг постоянный, то итерация сходится линейно.
Если же использовать переменный сдвиг
\begin{equation}
  \label{eq:var-del}
  \left(A - \tilde{\lambda_k}^{\left(s\right)} I\right) \vec{x}^{\left(s+1\right)} = \vec{x}^{\left(s\right)},
\end{equation}
$$
  \tilde{\lambda_k}^{\left(s+1\right)} = \tilde{\lambda_k}^{\left(s\right)} + \genfrac{<}{>}{1pt}{0}{x_i^{\left(s\right)}}{x_i^{\left(s+1\right)}}$$

Отметим, что формула (\ref{eq:var-del}) с учётом нормировки лучше записывается в виде
\[
  \left(A - \tilde{\lambda_k}^{\left(s\right)} I \right) \vec{y}^{\left(s\right)} = \vec{x}^{\left(s)\right)}
\]
\[
  \lambda_k^{\left(s+1\right)} = \lambda_k^{\left(s\right)} + \genfrac{<}{>}{1pt}{0}{x_i^{\left(s\right)}}{y_i^{\left(s\right)}}
\]
\begin{equation}
  \label{eq:another-var-del}
  \vec{x}^{\left(s+1\right)} = \frac{\vec{y}^{\left(s\right)}}{\left\| \vec{y}^{\left(s\right)} \right\|}
\end{equation}

Обратные итерации с постоянным и, особенно, с переменным сдвигом --- очень эффективный метод расчёта.
Для нахождения собственных векторов этот метод считается наиболее точным.
Сходимость при хорошем подборе $\tilde{\lambda}$ настолько быстрая, что метод пригоден для близких по модулю собственных значений.
\subsection{Преобразование Хаусхолдера}
\label{ss:hausholder}
Преобразование Хаусхолдера позволяет обратить в нуль группу поддиагональных элементов столбца матрицы.
Оно осуществляется с использованием матрицы Хаусхолдера, имеющий следующий вид:
\begin{equation}
\label{eq:hausholder-matrix}
H = I - \frac{2}{v^T v} v v^T
\end{equation}
, где $v$ --- произвольный ненулевой вектор-столбец, $I$ --- единичная матрица, $v v^T$ --- квадратная матрица того же размера.

Любая матрица такого вида является симметричной и ортогональной.
При этом произвол в выборе вектора $v$ даёт возможность построить матрицу, отвечающую некоторым дополнительным требованиям.

Рассмотрим случай, когда необходимо обратить в нуль все элементы какого-либо вектора, кроме первого.
Т.\,е. построить матрицу Хаусхолдера $H$ такую, что
$$\tilde{b} = H b, \qquad b = \left(b1, b2, \dots, b_n\right)^T, \qquad \tilde{b} = \left(\tilde{b_1}, 0, \dots, 0\right)^T$$

Тогда вектор $v$ определится следующим образом:
$$v = b + \sign{b_1}\left\|b\right\|_2 e_1$$
, где $\left\|b\right\|_2 = \sqrt{\sum\limits_i{b_i^2}}$ --- евклидова норма вектора, $e_1 = \left(1, 0, \dots, 0\right)^T$.

\subsection{QR-алгоритм}
\label{ss:qr}
При решении полной проблемы собственных значений эффективным является подход, основанный на приведении матриц к подобным, имеющим треугольный или квазитреугольный вид.
Одним из наиболее точных распространённых
методов этого класса является QR-алгоритм, позволяющий находить как вещественные, так и комплексные собственные значения.

В основе QR-алгоритма лежит представление матрицы $A$ в виде произведения двух матриц $QR$, где $Q$ --- ортогональная матрица, а $R$ --- верхнетреугольная.
Такое разложение существует для любой квадратной невырожденной матрицы.
Одним из подходов к построению QR-разложения является использование преобразования Хаусхолдера.

Применяя преобразования Хаусхолдера (см.~\ref{ss:hausholder}) с целью обнуления поддиагональных элементов каждого из столбцов исходной матрицы, можно за фиксированное число шагов получить её QR-разложение.

Чтобы получить QR-разложение, необходимо использовать преобразование Хаусхолдера $n-1$ раз.
После обнуления всех поддиагональных элементов мы получим следующее равенство:
$$R = H_{n-1} H_{n-2}\cdots H_1 A$$,
или
$$Q = H_1 H_2 \cdots H_{n-1}, A = QR$$

Т.\,к. каждая из матриц $H_i$ --- ортогональная, то матрица $Q$ --- также ортогональная.
Следовательно, следующая матрица будет подобна $A$:
$$A = Q^{-1} A Q = Q^{-1} Q R Q = R Q$$

После того, как получено разложение, строится следующий итерационный процесс:
\begin{enumerate}
\item $A^{\left(0\right)} = A$,
\item $A^{\left(0\right)} = Q^{\left(0\right)}R^{\left(0\right)}$ --- производится QR-разложение.
\item $A^{\left(1\right)} = R^{\left(0\right)}Q^{\left(0\right)}$ --- производится перемножение матриц (матрица $A^{\left(1\right)}$ подобна $A^{\left(0\right)}$.
\item \dots
\item $A^{\left(k\right)} = Q^{\left(k\right)}R^{\left(k\right)}$ --- QR-разложение.
\item $A^{\left(k+1\right)} = R^{\left(k\right)}Q^{\left(k\right)}$ --- перемножение.
\end{enumerate}

Таким образом, каждая итерация реализуется в два этапа.
На первом этапе осуществляется разложение матрицы $A^{\left(k\right)}$ в произведение ортогональной $Q^{\left(k\right)}$ и верхнетреугольной $R^{\left(k\right)}$ матриц, а на втором --- полученные матрицы перемножаются в обратном порядке.

\paragraph{Замечание}
Существенным недостатком рассмотренного выше алгоритма является большое число операций ($O(n^3)$), необходимое для QR-факторизации матрицы на каждой итерации.
Эффективность алгоритма может быть повышена, если предварительно с помощью преобразования подобия привести матрицу к верхней Хессенберговой форме:
$$
A^{\left(0\right)} = \left(\begin{matrix}
a_{11} & a_{12} & a_{13} & \hdots & a_{1 n-1} & a_{1n}\\
a_{21} & a_{22} & a_{23} & \hdots & a_{2 n-1} & a_{2n}\\
0 & 	a_{32} & a_{33} & \hdots & a_{3 n-1} & a_{3n}\\
\vdots & \vdots & \ddots & \ddots & \vdots & \vdots\\
0 & 0 & 0 & \hdots & a_{n-1 n-1} & a_{n-1 n}\\
0 & 0 & 0 & \hdots & a_{n n-1} & a_{nn} 
\end{matrix}\right)$$
\newpage
\section{Организация и выполнение вычислений}
\subsection{Поиск области, содержащую собственные числа}
Используя теорему Гершгорина (см.~\ref{ss:gershgorin}), получим
\[
\begin{aligned}
R_1 = \left|-2.0\right|+\left|0.6\right|+\left|0.2\right| = 2.8\qquad &\Rightarrow \left|\lambda - 7.2\right| \leqslant 2.8 \Rightarrow \lambda\in\left[4.4; 10\right]\\
R_2 = \left|-2.0\right|+\left|2.0\right|+\left|-1.5\right| = 5.5\qquad &\Rightarrow \left|\lambda - 7.3\right|\leqslant 5.5 \Rightarrow \lambda\in\left[1.8; 12.8\right]\\
R_3 = \left|0.6\right|+\left|2.0\right|=2.6\qquad &\Rightarrow \left|\lambda - 7.5\right|\leqslant 2.6 \Rightarrow \lambda\in\left[4.9; 10.1\right]\\
R_4 = \left|0.2\right|+\left|-1.5\right| = 1.7\qquad &\Rightarrow \left|\lambda - 8.0\right|\leqslant 1.7 \Rightarrow \lambda\in\left[6.3; 9.7\right]
\end{aligned}
\]
Т.\,к. все отрезки пересекаются, все собственные числа принадлежат отрезку $\left[1.8; 12.8\right]$.\

\subsection{Поиск характеристического многочлена}
Т.\,к. матрица $A$ симметрическая, оптимальным методом для построения характеристического многочлена является метод вращений (Якоби) (см. \ref{ss:jacobi})

Будем использовать следующие матрицы преобразований:
\[T_{23} = \left(\begin{matrix}
1 & 0 & 0 & 0\\
0 & 0.9578 & 0.2873 & 0\\
0 & -0.2873 & 0.9578 & 0\\
0 & 0 & 0 & 1
\end{matrix}\right)\]
\[
T_{24} = \left(
\begin{matrix}
1 & 0 & 0 & 0\\
0 & 0.9954 & 0 & 0.0954\\
0 & 0 & 1 & 0\\
0 & -0.0954 & 0 & 0.9954
\end{matrix}
\right)
\]
\[
T_{34} = \left(
\begin{matrix}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & -0.7219 & -0.6920\\
0 & 0 & 0.6920 & -0.7219
\end{matrix}
\right)
\]

Преобразуем исходную матрицу по формуле:
$$S = T_{34}^{-1} T_{24}^{-1} T_{23}^{-1} A T_{23} T_{24} T_{34}$$
Получим матрицу
\[
S = \left(
\begin{matrix}
7.2 & -2.0976 & 0 & 0\\
2.0976 & 6.5046 & -2.2833 & 0\\
0 & -2.2833  &  8.4411 & 0.4246\\
0 & 0 & 0.4246  & 7.8544
\end{matrix}
\right)
\]
Построим характеристический многочлен, используя рекуррентную формулу (\ref{eq:char-eq-formulae}).
\[
\begin{aligned}
\varphi_0\left(\lambda\right) &= 1\\
\varphi_1\left(\lambda\right) &= \lambda - 7.3\\
\varphi_2\left(\lambda\right) &= \left(\lambda - 6.5046\right)\varphi_1\left(\lambda\right) - (2.0976)^2 \varphi_0\left(\lambda\right)\\
\varphi_3\left(\lambda\right) &= \left(\lambda - 8.4411\right)\varphi_2\left(\lambda\right) - (2.2833)^2 \varphi_1\left(\lambda\right)\\
\varphi_4\left(\lambda\right) &= \left(\lambda - 7.8544\right)\varphi_3\left(\lambda\right) - (0.4246)^2 \varphi_2\left(\lambda\right)\\
\varphi\left(\lambda\right) &= (-1)^4 \varphi_4\left(\lambda\right)
\end{aligned}
\]
После упрощений, получим
\[
\varphi\left(\lambda\right) = \lambda^4 - 30 \lambda^3 + 326.66\lambda^2 -1519.1\lambda + 2510.78
\]

\subsection{Поиск собственного числа из интервала и соответствующего ему собственного вектора}
\[\lambda\in\left(10;12\right),\qquad \varepsilon_1 = 0.1\]
\[
\varphi\left(\lambda\right) = \lambda^4 - 30 \lambda^3 + 326.66\lambda^2 -1519.1\lambda + 2510.78
\]
\[
\varphi^\prime\left(\lambda\right) = 4\lambda^3 - 90\lambda^2 + 653.32\lambda - 1519.1
\]
Найдём приближённой значение $\lambda$ с помощью метода дихотомии:

\begin{tabular}{c|c}
$\lambda$ & $\strut\varphi\left(\lambda\right)$\\
\hline
$10$ & $-$\\
\hline
$10.25$ & $-$\\
\hline
$10.5$ & $+$\\
\hline
$11$ & $+$
\end{tabular}

\[
\lambda^{\left(1\right)} = 10.25,\qquad \lambda^{\left(2\right)} = 10.375
\]
\[m_1 = \min\limits_{\left[10.25; 10.5\right]}\left|\varphi^\prime\left(\lambda^{\left(2\right)}\right)\right| = 38.52\]
\[\frac{\left|\varphi\left(\lambda^{\left(2\right)}\right)\right|}{m_1} = \frac{\left|-4.692\right|}{38.52} \approx 0.122 > \varepsilon_1\]

\begin{tabular}{c|c}
$\lambda$ & $\varphi\left(\lambda\right)$\\
\hline
$10.25$ & $-$\\
\hline
$10.375$ & $-$\\
\hline
$10.5$ & $+$\\
\end{tabular}

\[\lambda^{\left(3\right)} = 10.438\]
\[m_1 = \min\limits_{\left[10.375; 10.5\right]}\left|\varphi^\prime\left(\lambda^{\left(3\right)}\right)\right| = 43.5\]
\[\frac{\left|\varphi\left(\lambda^{\left(3\right)}\right)\right|}{m_1} = \frac{\left|-2.109\right|}{43.5}\approx 0.049 < \varepsilon_1\]

Приближение $\lambda^{\left(3\right)}$ удовлетворяет точности $\varepsilon_1$.

\subsubsection{Уточнение собственного числа и поиск собственного вектора}
$\varepsilon_2 = 0.001$ --- точность для собственного числа $\lambda$,
$\varepsilon_3 = 0.01$ --- точность для соответствующего собственного вектора.

Воспользуемся обратным степенным методом со сдвигом (см.~\ref{ss:reverse}).

$\tilde\lambda = 10.438$

\[
\tilde{A} = A - \tilde\lambda I = \left(
\begin{matrix}
-3.238 & -2 & 0.6 & 0.2\\
-2 & -3.138 & 2 & -1.5\\
0.6 & 2 & -2.938 & 0\\
0.2 & -1.5 & 0 & -2.438
\end{matrix}
\right)
\]
\[
\det{\tilde{A}} = \left|
\begin{matrix}
-3.238 & -2 & 0.6 & 0.2\\
-2 & -3.138 & 2 & -1.5\\
0.6 & 2 & -2.938 & 0\\
0.2 & -1.5 & 0 & -2.438
\end{matrix}\right| = -2.109\]
\[
\tilde{A}^{-1} = \left(
\begin{matrix}
2.89959 & -5.824 & -3.382 & 3.821\\
-5.824 & 10.527 & 5.976 & -6.954\\
-3.372 & 5.976 & 3.039 & -3.954\\
3.821 & -6.954 & -3.954 & 4.182
\end{matrix}
\right)
\]

Возьмём произвольный ненулевой вектор:
\[y^{\left(0\right)} = \left(\begin{matrix}
1\\
1\\
1\\
1
\end{matrix}\right)\]
\[y^{\left(1\right)} = \tilde{A}^{-1} y^{\left(0\right)} = \left(\begin{matrix}
-2.476\\
3.725\\
1.67\\
-2.904
\end{matrix}\right)
\]
\[\left\|y^{\left(1\right)}\right\| = 5.595\]

\[
x^{\left(1\right)} = \frac{y^{\left(1\right)}}{\left\|y^{\left(1\right)}\right\|} = \left(\begin{matrix}
-0.443\\
0.666\\
0.362\\
-0.519
\end{matrix}
\right)
\]
\[
\bar{x}^{\left(1\right)} = \frac{\sum\limits_{i=1}^4{x_i^{\left(1\right)} }}{4} = 0.0015
\]
\[
\bar{y}^{\left(1\right)}  = \frac{\sum\limits_{i=1}^4{y_i^{\left(1\right)} }}{4} = 0.0084
\]
\[
\lambda^{\left(1\right)} = \tilde{\lambda} + \frac{\bar{x}^{\left(1\right)} }{\bar{y}^{\left(1\right)} } = 10.616754
\]
\[
y^{\left(2\right)} = \tilde{A}^{-1} y^{\left(1\right)}  = \left(\begin{matrix}
-8.164\\
15.002\\
8.443\\
-9.687
\end{matrix}
\right)
\]
\[\left\|y^{\left(2\right)} \right\| = 21.374\]
\[x^{\left(2\right)} = \frac{y^{\left(2\right)} }{\left\|y^{\left(2\right)} \right\|}=\left(\begin{matrix}
-0.382\\
0.702\\
0.395\\
-0.453
\end{matrix}\right)\]
\[\bar{x}^{\left(2\right)} = \frac{\sum\limits_{i=1}^4{x_i^{\left(2\right)} }}{4} = 1.3986,\qquad\left|\bar{x}^{\left(2\right)} - \bar{x}^{\left(1\right)} \right| = \left|0.0654 - 0.0015\right| > \varepsilon_3\]
\[\lambda^{\left(2\right)} = \tilde{\lambda} = \frac{\bar{x}^{\left(2\right)} }{\bar{y}^{\left(2\right)} } =10.4848, \qquad \left|\lambda^{\left(2\right)} - \lambda^{\left(1\right)} \right| = \left|10.48484 - 10.616754\right| > \varepsilon_2\]
\[y^{\left(3\right)}  = \tilde{A}^{-1} y^{\left(2\right)}  = \left(\begin{matrix}
-8.2594\\
15.126\\
8.476\\
-9.798
\end{matrix}\right)\]
\[\left\|y^{\left(3\right)} \right\| = 21.56\]
\[x^{\left(3\right)} = \frac{y^{\left(3\right)} }{\left\|y^{\left(3\right)} \right\|} = \left(\begin{matrix}
-0.383\\
0.702\\
0.393\\
-0.4544
\end{matrix}
\right)\]
\[\bar{x}^{\left(3\right)} = \frac{\sum\limits_{i=1}^4{x_i^{\left(3\right)} }}{4} = 0.0643\]
\[\bar{y}^{\left(3\right)} = \frac{\sum\limits_{i=1}^4{y_i^{\left(3\right)} }}{4} = 1.386\]
\[\lambda^{\left(3\right)}  = \tilde{\lambda} + \frac{\bar{x}^{\left(3\right)} }{\bar{y}^{\left(3\right)} } = 10.48438\]
\[\left|\lambda^{\left(3\right)} - \lambda^{\left(2\right)} \right| = \left|10.48438 - 10.4848\right| = 0.00042 < \varepsilon_2\]
$\lambda \approx 10.484$ --- собственное число
\[\left|\bar{x}^{\left(3\right)} - \bar{x}^{\left(2\right)} \right| = \left|0.0643 - 0.0654\right| = 0.0011 < \varepsilon_3\]
$x = x^{\left(3\right)}  = \left(\begin{matrix}
-0.383\\
0.762\\
0.393\\
-0.4544
\end{matrix}\right)$ --- собственный вектор, соответствующий собственному числу $\lambda = 10.484$:

\[
A x = \left( \begin{matrix}
7.2 & -2.0 & 0.6 & 0.2\\
-2.0 & 7.3 & 2.0 & -1.5\\
0.6 & 2.0 & 7.5 & 0\\
0.2 & -1.5 & 0 & 8.0
\end{matrix}\right) \left(\begin{matrix}
-0.383\\
0.702\\
0.393\\
-0.4544
\end{matrix}\right) \approx \left(\begin{matrix}
-4.016\\
7.355\\
4.122\\
-4.765
\end{matrix}\right)\]

\[\lambda x \approx \left(\begin{matrix}
-4.016\\
7.355\\
4.121\\
-4.765
\end{matrix}\right)\]
\subsection{QR-алгоритм}
Результаты QR-алгоритма в виде таблицы:

\begin{landscape}
\begin{tabular}{|l|c|c|c|}
\hline
$n$ & $A^{\left(n\right)}$ & $Q^{\left(n\right)}$ & $R^{\left(n\right)}$\\
\hline
$0$ & $\left(
\begin{matrix}
7.2 & -2.0976 & 0 & 0\\
2.0976 & 6.5046 & -2.2833 & 0\\
0 & -2.2833  &  8.4411 & 0.4246\\
0 & 0 & 0.4246  & 7.8544
\end{matrix}
\right)$ &
$\left(
\begin{matrix}
-0.9600 &  -0.2594 & -0.1045 & -0.0063\\
0.2797 &  -0.8903 &  -0.3586 &  -0.0217\\
 0 &  0.3742 &  -0.9256 &  -0.0561\\
 0 & 0 &  -0.0605 &   0.9982
\end{matrix}
\right)$ & 
$\left(
\begin{matrix}
 -7.4993 &   3.8333 &  -0.6387  &  0\\
0 & -6.1015 &   5.1918 & 0.1589\\
0 & 0  & -7.0202  & -0.8681\\
0 & 0  & 0 &  7.8162
\end{matrix}
\right)$\\
\hline
$1$ & $\strut\left(
\begin{matrix}
8.2722 &   -1.7067 &   0 &   0\\
-1.7067 &   7.3752 &  -2.6271 & 0\\
0 & -2.6271 &   6.5507 & -0.4728\\
0 &  0 & -0.4728  & 7.8019
\end{matrix}
\right)$
&
$\left(
\begin{matrix}
 -0.9794 &  -0.1888 &  -0.0718 &   0.0065\\
    0.2021 &  -0.9149 &  -0.348 &   0.0316\\
     0 &  0.3568 & -0.9303 &  0.0846\\
   0 &   0  & 0.0905 &  0.9959
\end{matrix}
\right)$
&
$\left(
\begin{matrix}
 -8.4464   & 3.1617 & -0.5308 &  0\\
  0 & -7.3629 &  4.7409 & -0.1687\\
  0 & 0 & -5.223  & 1.146\\
  0 & 0 & 0 &  7.7299
\end{matrix}
\right)$\\
\hline
$2$ &
$\left(
\begin{matrix}
 8.9110 & -1.4877 &   0 &   0\\
   -1.4877 &  8.428 & -1.8636 &  0\\
   0 & -1.8636 &  4.9629 &  0.6997\\
   0 &  0  & 0.6997 &  7.6981
\end{matrix}
\right)$ &
$\left(
\begin{matrix}
 -0.9864 & -0.1604 & -0.0366 & -0.0058\\
   0.1647 & -0.961 & -0.2194 & -0.0347\\
   0 &  0.2251 & -0.9624 & -0.1523\\
   0  & 0 & -0.1563 &  0.9877
\end{matrix}
\right)$
&
$\left(
\begin{matrix}
 -9.0344 &  2.8553 &  -0.3069 &   0\\
  0 & -8.2804 &  2.9079 &   0.1575\\
  0 &  0 & -4.4769 & -1.8765\\
  0 & 0 & 0 &   7.497
\end{matrix}
\right)$\\
\hline
$3$ &
$\left(
\begin{matrix}
 9.3812 & -1.3636 &  0 &   0\\
   -1.3636 &  8.6123 & -1.0076 &  0\\
   0 & -1.0076 &  4.6017 & -1.1717\\
   0  & 0 & -1.1717 &  7.4048
\end{matrix}
\right)$ &
$\left(
\begin{matrix}
-0.9896 & -0.1428 & -0.0167 &  0.0044\\
   0.1438 & -0.9824 & -0.115 &  0.0303\\
   0 &  0.1201 & -0.96 &  0.2529\\
   0 &  0  & 0.2547 &  0.967
\end{matrix}
\right)$ &
$\left(
\begin{matrix}
 -9.4798 &  2.5882 & -0.1449 &   0\\
  0 & -8.3873 &  1.5427 & -0.1408\\
  0 &  0 & -4.6003 &  3.0109\\
  0 & 0 & 0 &  6.8644
\end{matrix}
\right)$\\
\hline
\dots & \dots & \dots & \dots \\
\hline
$10$ &
$\left(
\begin{matrix}
10.4548 & -0.2761 &   0 &   0\\
   -0.2761 &   7.8865 &  -0.2496 &  0\\
0 &   -0.2496 &   7.6892 &   0.1062\\
0  &  0  &  0.1062  &  3.9695
\end{matrix}
\right)$ &
$\left(
\begin{matrix}
-0.9997 & -0.0264 & -0.0008 & 0\\
   0.0264 & -0.9992 & -0.0317 & -0.0004\\
   0 & 0.0317 & -0.9994 &  -0.0138\\
   0 &  0 &  -0.0138 &  0.9999
\end{matrix}
\right)$ &
$\left(
\begin{matrix}
-10.4584 &  0.4842 &  -0.0066 &   0\\
 0 &  -7.8804 &   0.493 &   0.0034\\
 0 & 0 &  -7.6782  & -0.161\\
    0 & 0 & 0 & 3.9677
\end{matrix}
\right)$\\
\hline
\dots & \dots & \dots & \dots\\
\hline
$30$ &
$\left(
\begin{matrix}
 10.4844 &   -0.0011 &   0  &  0\\
   -0.0011 &   8.0183 &  -0.0948 &   0\\
    0 &  -0.0948 &   7.5309  &  0\\
    0 &   0 &   0 &   3.9665
\end{matrix}
\right)$ &
$\left(
\begin{matrix}
 -1 &  -0.0001 & 0 & 0\\
 0.0001 & -0.9999 & -0.0118 & 0\\
   0 &  0.0118 & -0.9999 & 0\\
   0 &  0 & 0 &  1
\end{matrix}
\right)$ &
$\left(
\begin{matrix}
 -10.4844 &  0.002 &  0 &   0\\
   0 &  -8.0188 &   0.1839 & 0\\
   0 &   0 & -7.5292 &  0\\
   0 &   0 &  0 &   3.9665
\end{matrix}
\right)$\\
\hline
\end{tabular}
\end{landscape}

Проведём анализ этих итераций:
\begin{enumerate}
\item Видны два блока: $\left(\begin{matrix}
8.2722 & -1.7067\\
-1.7067 & 7.3752
\end{matrix}\right)$ и $\left(\begin{matrix}
6.5507 & -0.4728\\
-0.4728 & 7.8019
\end{matrix}\right)$, однако внедиагональные элементы достаточно большие
\item Значения в вышеупомянутых блоках немного изменились, внедиагональные элементы уменьшились (по модулю)
\item Блоки снова немного изменились, внедиагональные элементы $\approx 1$.
Посчитаем собственные числа для верхнего блока:
\[\left(9.3812 - \lambda\right)\left(8.6123 - \lambda\right) - 1.3636^2 = 0 \Rightarrow \lambda_1 = 7.58, \lambda_2 = 10.4135\]
Подсчитаем собственные числа для нижнего блока:
\[\left(4.6017 - \lambda\right)\left(7.4048 - \lambda\right) - 1.1717^2 = 0 \Rightarrow \lambda_3 = 4.1764, \lambda_4 = 7.8401\]
Как видно, значение $\lambda_2$ наиболее близко к значению $\lambda$, полученного в предыдущем пункте, однако разница между ними $\approx 0.07$.
\item[10.] Внедиагональные элементы на порядок меньше диагональных.
\item[30.] Матрица практически диагональная, с очень маленькими внедиагональными элементами. Элемент матрицы $a^{\left(30\right)}_{11}$ с большой точностью является собственным значением, полученным в предыдущем пункте.
\end{enumerate}
На дальнейших итерациях диагональные элементы практически не меняются, постепенно вырождаются в нуль внедиагональные элементы.
\newpage
\section{Анализ полученных результатов}
При выполнении данной курсовой работы были рассмотрены численные методы нахождения собственных чисел и векторов матрицы, построения её характеристического многочлена, операцию преобразования подобия и QR-разложения матрицы.
Практические результаты подтвердили теоретические сведения:
\begin{itemize}
\item Показано, что теорема Гершгорина даёт быструю и точную оценку интервала, содержащего собственные числа.
\item Закреплены практические навыки преобразования подобия матриц: метод Хаусхолдера и матрицы вращений.
\item Подтверждён высокий уровень точности построения характеристического многочлена при использовании метода Якоби.
\item Показан блочный вид матрицы, полученной применением QR-алгоритма.
\item Подтверждено увеличение быстродействия QR-алгоритма при начальном преобразовании исходной матрицы в подобной ей матрицы верхней Гессенбергской формы.
\item Замечено быстрая сходимость обратного степенного метода со сдвигом при достаточно высокой точности полученных значений.
\end{itemize}
\newpage
\addcontentsline{toc}{section}{Литература}
\begin{thebibliography}{0}
\bibitem{Yakut}
Якут Л.\,И., Резников И.\,Ф. Методические указания к курсовым работам по дисциплине Численные методы. Киев: КПИ, 1982. --- 63с.
\bibitem{Kalitkin}
Калиткин Н.\,Н. Численные методы. Москва: Наука, 1978. --- 508с.
\bibitem{NumRecipes}
Press H.\,W., Teukolsky S.\,A., Vetterling W.\,T., Flannery B.\,P. Numerical recipes. The art of scientific computing. Third edition. Cambridge: 2007. --- 1262p.
\end{thebibliography}
\newpage
\section*{Приложения}
\addcontentsline{toc}{section}{Приложения}
\subsection*{Исходный код преобразования матрицы в верхнюю Гессенбергскую форму}
\listinginput{1}{generate_hessenberg_matrix.m}
\subsection*{Исходный код реализации QR-разложения}
\listinginput{1}{calculate_qr.m}
\subsection*{Исходный код QR-алгоритма}
\listinginput{1}{numm.m}
\end{document}



















